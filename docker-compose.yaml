# version: '3.7'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - shortlink_net
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    # healthcheck: 健康检查配置有问题
    #   test: ["CMD", "zkServer.sh", "status"]
    #   interval: 30s
    #   timeout: 10s
    #   retries: 3
    #   start_period: 40s

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    hostname: kafka1
    container_name: kafka1
    ports:
      - "9092:9092"
    depends_on:
      - zookeeper
        # condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,DOCKER:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,DOCKER://kafka1:9093
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,DOCKER://0.0.0.0:9093
      KAFKA_INTER_BROKER_LISTENER_NAME: DOCKER
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      # KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_CREATE_TOPICS: "shortlink-log:3:1"
    networks:
      - shortlink_net
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  nginx:
    image: nginx:latest
    container_name: nginx
    ports:
      - "80:80"  # 将宿主机 80 映射到容器的 80
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    extra_hosts:
      - "host.docker.internal:host-gateway"  # Linux 特别需要的配置
    networks:
      - shortlink_net
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 30s
      timeout: 10s
      retries: 3

  nacos:
    image: nacos/nacos-server:v2.2.3
    container_name: nacos
    hostname: nacos
    environment:
      MODE: standalone
      NACOS_AUTH_ENABLE: "false"
      # NACOS_SERVER_IP: 0.0.0.0    # ✅ 监听所有地址
    ports:
      - "8848:8848"   # Nacos 控制台
      - "9848:9848"  # 必须要把这两个端口暴露出来
      - "9849:9849"
    networks:
      - shortlink_net
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8848/nacos"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s


  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.1
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"  # 设置 JVM 内存
    ports:
      - "9200:9200"  # Elasticsearch HTTP 接口
      - "9300:9300"  # 集群通讯（暂不使用）
    networks:
      - shortlink_net
    restart: unless-stopped
    volumes:
      - esdata:/usr/share/elasticsearch/data

  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.1
    container_name: kibana
    depends_on:
      - elasticsearch
    ports:
      - "5601:5601"  # Kibana Web UI
    environment:
      ELASTICSEARCH_HOSTS: "http://elasticsearch:9200"
    networks:
      - shortlink_net
    restart: unless-stopped

  logstash:
    image: docker.elastic.co/logstash/logstash:8.11.1
    container_name: logstash
    depends_on:
      - elasticsearch
    volumes:
      - ./logservice/logstash/pipeline:/usr/share/logstash/pipeline  # 放置 logstash.conf
    ports:
      - "5044:5044"    # Logstash beats 输入端口（如 Filebeat）
      - "9600:9600"    # Logstash HTTP API
    networks:
      - shortlink_net
    environment:
      LS_JAVA_OPTS: "-Xms512m -Xmx512m"
    restart: unless-stopped

volumes:
  esdata:

networks:
  shortlink_net:
    driver: bridge